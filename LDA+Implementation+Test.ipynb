{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "from scipy.special import digamma\n",
    "from scipy.special import polygamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doc params\n",
    "V = 5 # Number of unique possible words\n",
    "N = 10 # Number of words PER DOCUMENT (here all docs have same number of words)\n",
    "K = 2 # Number of topics\n",
    "M = 10 # Number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set corpus parameters\n",
    "alpha_true = np.array([5, 3]) # Parameters for sampling topic distribution\n",
    "beta_true = np.zeros((K, V)) # Parameters for sampling word distribution per topic\n",
    "beta_true[0, :] = np.array([0.7, 0.2, 0.1, 0, 0])\n",
    "beta_true[1, :] = np.array([0, 0, 0.3, 0.3, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67070431,  0.32929569],\n",
       "       [ 0.78771623,  0.21228377],\n",
       "       [ 0.64892707,  0.35107293],\n",
       "       [ 0.52370171,  0.47629829],\n",
       "       [ 0.59934066,  0.40065934],\n",
       "       [ 0.87273546,  0.12726454],\n",
       "       [ 0.37528339,  0.62471661],\n",
       "       [ 0.46307675,  0.53692325],\n",
       "       [ 0.68783733,  0.31216267],\n",
       "       [ 0.7164995 ,  0.2835005 ]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate theta, topic distributions for each doc\n",
    "theta = np.random.dirichlet(alpha_true, size = M)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data for one document\n",
    "z = np.zeros((M, N), dtype = int) # Sample topics\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        z[i, j] = np.nonzero(np.random.multinomial(n = 1, pvals = theta[i,]))[0]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 4, 0, 3, 0, 0, 0, 0],\n",
       "       [4, 1, 0, 3, 2, 0, 4, 0, 0, 3],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 3, 0, 0],\n",
       "       [2, 3, 1, 0, 2, 4, 4, 1, 4, 1],\n",
       "       [1, 2, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 3, 0, 0, 3, 0, 0, 0, 2, 0],\n",
       "       [4, 2, 2, 1, 2, 4, 2, 2, 4, 3],\n",
       "       [1, 3, 3, 4, 3, 2, 0, 3, 4, 0],\n",
       "       [0, 3, 0, 0, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 3, 2, 0, 1, 0, 2, 0, 2]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.zeros((M, N), dtype = int) # Sample words\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        w[i, j] = np.nonzero(np.random.multinomial(n = 1, pvals = beta_true[z[i, j], :]))[0]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B = np.zeros((N, V, M)) # Wordbank, where each row is a 1-hot vector\n",
    "for i in range(M):\n",
    "    for n in range(N):\n",
    "        B[n, w[i, n], i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# E-step, update phi and gamma\n",
    "def E_step(alpha, beta, w, N, K, tol = 1e-6):\n",
    "    phi_old = np.ones((N, K))\n",
    "    phi_old /= K\n",
    "    phi_new = np.ones((N, K))\n",
    "    gamma_old = alpha + N / K\n",
    "    diff = 10\n",
    "    while diff > tol:\n",
    "        for n in range(N):\n",
    "            for k in range(K):\n",
    "                phi_new[n, k] = beta[k, w[n]]*np.exp(digamma(gamma_old[k]) - digamma(sum(gamma_old)))\n",
    "            phi_new[n, :] /= sum(phi_new[n, :])\n",
    "        gamma_new = alpha + np.sum(phi_new, axis = 0)\n",
    "        diff = linalg.norm(gamma_new - gamma_old) + linalg.norm(phi_new - phi_old)\n",
    "        gamma_old = gamma_new\n",
    "        phi_old = phi_new\n",
    "    return (gamma_new, phi_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update beta\n",
    "def up_beta(phi, B, K, V, M):\n",
    "    vec = np.zeros((K, V))\n",
    "    for k in range(K):\n",
    "        for v in range(V):\n",
    "            for m in range(M):\n",
    "                vec[k, v] += sum(phi[:, k, m]*B[:, v, m])\n",
    "        vec[k, :] /= sum(vec[k, :])\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad(alpha, gamma, M):\n",
    "    \"\"\"\"\"\"\n",
    "    g = np.zeros((len(alpha)))\n",
    "    for i in range(len(alpha)):\n",
    "        g[i] = M*(digamma(sum(alpha)) - digamma(alpha[i])) + sum(digamma(gamma[i, :]) - digamma(sum(gamma)))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Hessian(alpha, M):\n",
    "    \"\"\"\"\"\"\n",
    "    H = np.zeros((len(alpha), len(alpha)))\n",
    "    for i in range(len(alpha)):\n",
    "        for j in range(len(alpha)):\n",
    "            if i == j:\n",
    "                H[i, j] = M*(polygamma(1, sum(alpha)) - polygamma(1, alpha[i]))\n",
    "            else:\n",
    "                H[i, j] = M*(polygamma(1, sum(alpha)))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update alpha\n",
    "def newton_raphson(g, H, a0, gamma, M, tol = 1e-4):\n",
    "    \"\"\"\"\"\"\n",
    "    diff = 10\n",
    "    a_old = a0\n",
    "    while diff > tol:\n",
    "        a_new = a_old - linalg.inv(H(a_old, M)) @ g(a_old, gamma, M)\n",
    "        diff = linalg.norm(a_new - a_old)\n",
    "        a_old = a_new\n",
    "        #print(a_new)\n",
    "    return a_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# M-step, update alpha and beta\n",
    "def M_step(phi, B, K, V, M, g, H, a0, gamma):\n",
    "    \"\"\"\"\"\"\n",
    "    #alpha = newton_raphson(g, H, a0, gamma, M)\n",
    "    beta = up_beta(phi, B, K, V, M)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement E-M algorithm\n",
    "alpha = alpha_true\n",
    "beta = np.ones((K, V))\n",
    "beta[0, :] = np.array([.2, .2, .2, .2, .2])\n",
    "beta[1, :] = np.array([.1, .3, 0.1, .3, .2])\n",
    "gamma = np.zeros((K, M))\n",
    "phi = np.zeros((N, K, M))\n",
    "\n",
    "it = 300 # Number of iterations of E-M algorithm\n",
    "for i in range(it):\n",
    "    for m in range(M):\n",
    "        doc = E_step(alpha, beta, w[m,], N, K)\n",
    "        gamma[:, m] = doc[0]\n",
    "        phi[:, :, m] = doc[1] \n",
    "    beta = M_step(phi, B, K, V, M, grad, Hessian, np.array([1, 1]), gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71,  0.23,  0.  ,  0.05,  0.  ],\n",
       "       [ 0.  ,  0.04,  0.39,  0.28,  0.29]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(beta, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7,  0.2,  0.1,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0.3,  0.3,  0.4]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
