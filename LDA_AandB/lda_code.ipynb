{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(w, K, M, V, doc_lens):\n",
    "    \"\"\"Initializes values for collapsed gibbs sampler\"\"\"\n",
    "    \n",
    "    # Set initial z randomly\n",
    "    z = {}\n",
    "    for m in range(M):\n",
    "        z[m] = []\n",
    "        for n in range(doc_lens[m]):\n",
    "            z[m].extend(np.nonzero(np.random.multinomial(1, np.ones(K)/K))[0])\n",
    "    \n",
    "    # Create count matrices\n",
    "    N_1 = np.zeros((M, K))\n",
    "    for m in range(M):\n",
    "        for k in range(K):\n",
    "            N_1[m, k] = sum(np.array(z[m]) == k)\n",
    "            \n",
    "    N_2 = np.zeros((K, V))\n",
    "    for m in range(M):\n",
    "        for n in range(doc_lens[m]):\n",
    "            N_2[z[m][n], w[m][n]] += 1\n",
    "            \n",
    "    N_3 = np.zeros(K)\n",
    "    for m in range(M):\n",
    "        for n in range(doc_lens[m]):\n",
    "            N_3[z[m][n]] += 1\n",
    "            \n",
    "    return((z, N_1, N_2, N_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(w, K, M, V, doc_lens, alpha, beta, N_1, N_2, N_3, z, n_iter):\n",
    "    \"\"\"Runs gibbs sampler to get estimated latent topics\"\"\"\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        for m in range(M):\n",
    "            for n in range(doc_lens[m]):\n",
    "                N_1[m, z[m][n]] -= 1\n",
    "                N_2[z[m][n], w[m][n]] -= 1\n",
    "                N_3[z[m][n]] -= 1\n",
    "                p = np.zeros(K)\n",
    "                for k in range(K):\n",
    "                    p[k] = (N_1[m, k] + alpha[k])*((N_2[k, w[m][n]] + beta[w[m][n]])/(N_3[k] + sum(beta)))\n",
    "                p /= sum(p)\n",
    "                z[m][n] = np.nonzero(np.random.multinomial(1, p))[0][0]\n",
    "                N_1[m, z[m][n]] += 1\n",
    "                N_2[z[m][n], w[m][n]] += 1\n",
    "                N_3[z[m][n]] += 1\n",
    "                \n",
    "    return((N_1, N_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_dist(N_1, doc_lens, alpha, M, K):\n",
    "    \"\"\"Calculates MC estimates for topic distributions using results from Gibbs sampler\"\"\"\n",
    "    \n",
    "    theta = np.zeros((M, K))\n",
    "    for m in range(M):\n",
    "        for k in range(K):\n",
    "            theta[m, k] = (N_1[m, k] + alpha[k])/(doc_lens[m] + sum(alpha))\n",
    "            \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_dist(N_2, beta, V, K):\n",
    "    \"\"\"Calculates MC estimates for word distributions using results from Gibbs sampler\"\"\"\n",
    "    \n",
    "    phi = np.zeros((K, V))\n",
    "    for k in range(K):\n",
    "        for v in range(V):\n",
    "            phi[k, v] = (N_2[k, v] + beta[v]) / (sum(N_2[k, :]) + sum(beta))\n",
    "            \n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(bow, K, alpha = 1, beta = 1, n_iter = 1000):\n",
    "    \"\"\"LDA implementation using collapsed Gibbs sampler\"\"\"\n",
    "    \n",
    "    # Get corpus parameters\n",
    "    M, V = bow.shape\n",
    "    doc_lens = np.sum(bow, axis = 1, dtype = 'int')\n",
    "    \n",
    "    # Create word dictionary\n",
    "    w = {}\n",
    "    for m in range(M):\n",
    "        w[m] = []\n",
    "        for v in range(V):\n",
    "            for n in range(int(bow[m, v])):\n",
    "                w[m].append(v)\n",
    "    \n",
    "    # Initialize values for Gibbs sampler   \n",
    "    z, N_1, N_2, N_3 = initialize(w, K, M, V, doc_lens)\n",
    "    \n",
    "    \n",
    "    # Set symmetric hyperparameters\n",
    "    alpha = np.ones(K) * alpha\n",
    "    beta  = np.ones(V) * beta\n",
    "    \n",
    "    # Run Gibbs sampler\n",
    "    N_1, N_2 = gibbs(w, K, M, V, doc_lens, alpha, beta, N_1, N_2, N_3, z, n_iter)\n",
    "    \n",
    "    # Estimate topic and word distributions\n",
    "    theta = topic_dist(N_1, doc_lens, alpha, M, K)\n",
    "    phi   = word_dist(N_2, beta, V, K)\n",
    "    \n",
    "    return((theta, phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc params\n",
    "V = 10\n",
    "N_min = 99\n",
    "N_max = 100\n",
    "K = 3\n",
    "M = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set true params\n",
    "alpha_true = np.array([2, 1, 2])\n",
    "beta_true = np.random.randint(1, 2, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "phi_true = np.zeros((K, V))\n",
    "for k in range(K):\n",
    "    phi_true[k, :] = np.random.dirichlet(beta_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_true = np.zeros((M, K))\n",
    "for m in range(M):\n",
    "    theta_true[m,:] = np.random.dirichlet(alpha_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = np.random.randint(N_min, N_max, M)\n",
    "z_true = {}\n",
    "w = {}\n",
    "for m in range(M):\n",
    "    z_true[m] = []\n",
    "    w[m] = []\n",
    "    for n in range(doc_lens[m]):\n",
    "        z_true[m].extend(np.nonzero(np.random.multinomial(1, theta_true[m,:]))[0])\n",
    "        w[m].extend(np.nonzero(np.random.multinomial(1, phi_true[z_true[m][n], :]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = np.zeros((M, V))\n",
    "for m in range(M):\n",
    "    for v in range(V):\n",
    "        bow[m, v] = len(np.where(np.array(w[m]) == v)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lda(bow, K, 1, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04, 0.02, 0.13, 0.07, 0.02, 0.02, 0.07, 0.22, 0.26, 0.15])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(results[1][1,:], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09, 0.24, 0.  , 0.06, 0.13, 0.07, 0.1 , 0.03, 0.19, 0.1 ])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(phi_true[1,:], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
