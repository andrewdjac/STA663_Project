{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Latent Dirichlet Allocation for Topic Modeling and Document Classification\n",
    "\n",
    "### Andrew Cooper, Brian Cozzi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we implement a form of Latent Dirichlet Allocation for topic modeling. Latent Dirichlet Allocation (LDA) was first introduced by Blei, Ng, and Jordan in 2003 as a hierarchical modeling approach for discrete data such as text in a corpus. This algorithm hinges on the notion that collections of data, such as text in a document, are generated according to a latent topic distribution, where each topic assigns probabilities to different words. The purpose of LDA in topic modeling is to group documents based on similar topic distributions, and to identify key words in each topic. Using a collapsed Gibbs sampler approach to LDA as described in Darling 2011, we implement an algorithm that estimates the latent topic distributions of a given corpus of documents. In addition, our algorithm returns key words assigned to each topic. We optimize our algorithm's Gibbs sampler using \"just-in-time\" compilation. We then evaluate the performance of our algorithm on both simulated data and documents from the Newsgroups corpus. Finally, we compare the accuracy of our algorithm to a variational bayes approach to LDA and to Latent Semantic Analysis (LSA).\n",
    "\n",
    "Key phrases: Latent Dirichlet Allocation, Topic Modeling, Collapsed Gibbs Sampler, Newsgroups Corpus, Variational Bayes, Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background (Brian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State the research paper you are using. Describe the concept of the algorithm and why it is interesting and/or useful. If appropriate, describe the mathematical basis of the algorithm. Some potential topics for the background include:\n",
    "* What problem does it address?\n",
    "* What are known and possible applications of the algorithm?\n",
    "* What are its advantages and disadvantages relative to other algorithms?\n",
    "* How will you use it in your research?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA algorithm takes in 4 inputs: a corpus of documents, the number of topics, two optional choices for hyperparameters, and an optional specification on the number iterations for the Gibbs Sampler. The ultimate goal of the algorithm is to estimate the topic distribution for each document as well as the word distribution for each topic. It does this by making inference on the latent topics of each word in the given corpus. We perform this inference by implementing a Gibbs sampler. First the algorithm sets a starting point by randomly sampling topics for each of the words in the corpus. Then it iteratively samples new topics for each word using calculated posterior probabilities. After a number of iterations, the Gibbs Sampler returns the estimated topics for each word, which are then used to calculate the latent topic distributions and word distributions using Monte Carlo estimation. These estimated quantities are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA algorithm has many different symbols and components. We establish all the symbols used in our algorithm below:\n",
    "\n",
    "* $K$ = The number of topics\n",
    "* $M$ = The number of documents in the corpus\n",
    "* $N_m$ = The number of words in document $m$\n",
    "* $V$ = The number of possible words\n",
    "* $w_{m,n}$ = The nth word in document $m$\n",
    "* $z_{m,n}$ = The nth topic in document $m$ \n",
    "* $\\theta_m$ = The topic distribution of document $m$\n",
    "* $\\phi_k$ = The word distribution of topic $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm takes as input a corpus of documents represented as a $M$x$V$  bag-of-words matrix. In addition, it takes as input the number of topics $K$. It also takes in two positive values representing the hyperparameters for the topic distribution ($\\alpha$) and the word distribution ($\\beta$). For this implementation we use symmetric priors for the dirichlet distributions, which means only one value is needed as input for each prior. Finally it takes as input the number iterations for the Gibbs sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for LDA has three main steps. The first step of the algorithm is preparing the data for the Gibbs sampler. As a starting point for our sampler, we first must randomly assign topics $z_{m,n}$ for each of the words in the given corpus. We then create two different count matrices, $N_1$ and $N_2$: $N_1$ is a $M$x$K$ matrix that counts the distribution of topics across documents. $N_2$ is a $K$x$V$ matrix that counts the distribution of words across topics. The count matrices are initialized according to the random topic assignment.\n",
    "\n",
    "The second step is the implementation of the Gibbs sampler. For each iteration, our sampler loops through every word $w_{m,n}$ in every document of our corpus. For each word, we first remove its assigned topic and decrement the appropriate count matrices $N_1$, $N_2$, and $N_3$. We then calculate the posterior probabilities of the word belonging to each of the possible topics. One of these topics is sampled using these probabilities and is assigned to the word. Finally, all of the count matrices are incremented according to this new topic. This process is done for all the words in the corpus for how many iterations the user specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gibbs Sampler Implementation***\n",
    "\n",
    "* Randomly assign topics $z_{m,n}$ for all words in corpus, and initialize count matrices $N_1$ and $N_2$\n",
    "\n",
    "* **for** $i = 1$ to n_iter:\n",
    "   * **for** $m = 1$ to $M$:\n",
    "       * **for** $n = 1$ to $N_m$:\n",
    "           * $w = w_{m,n}$\n",
    "           * $z_{old} = z_{m,n}$\n",
    "           * $N_1[m, z_{old}] -= 1$\n",
    "           * $N_2[z_{old}, w] -= 1$\n",
    "           * **for** $k = 1$ to $K$:\n",
    "               * $p_k = Pr(z = k | \\dots) \\propto (N_1[m, k] + \\alpha[k])*\\dfrac{N_2[k, w] + \\beta[w]}{\\sum_V{N_2[k, v] + \\beta[v]}}$\n",
    "           * Normalize $p$ \n",
    "           * $z_{new} = $ Sample from $Cat(K, p)$\n",
    "           * $z_{m,n} = z_{new}$\n",
    "           * $N_1[m, z_{new}] += 1$\n",
    "           * $N_2[z_{new}, w] += 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step to the algorithm is estimating and returning the quantities of interest. One quantity to estimate is the topic distribution $\\theta_m$ for each document $m$. Another quantity to estimate is the word distribution $\\phi_k$ for each topic. These quantities are estimated using the count matrices $N_1$ and $N_2$ established in the Gibbs sampler:\n",
    "\n",
    "$\\hat{\\theta}_{m,k} = \\dfrac{N_1[m, k] + \\alpha[k]}{\\sum_k{N_1[m, k] + \\alpha[k]}}$ \n",
    "\n",
    "$\\hat{\\phi}_{k, v} = \\dfrac{N_2[k, v] + \\beta[v]}{\\sum_v{N_2[k, v] + \\beta[v]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications to Simulated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the LDA framework, documents are assumed to be generated under the following stochastic process:\n",
    "\n",
    "For each document $m$, sample topic distribution $ \\theta_m \\sim Dirichlet(\\alpha)$\n",
    "\n",
    "For each topic $k$, sample word distribution $ \\phi_k \\sim Dirichlet(\\beta)$\n",
    "\n",
    "For each word $n$ in each document,\n",
    "\n",
    "1) Sample topic $z_n \\sim Cat(\\theta_m)$\n",
    "\n",
    "2) Sample word $w_n \\sim Cat(\\phi_{z_n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the correctness of our LDA algorithm, we simulate data under this stochastic process. We then train this data on our algorithm and compare the parameter estimates to the true parameters.\n",
    "\n",
    "We simulate a corpus of 10 documents containing 100 unique \"words\". Documents in the corpus are composed of 2 different topics and contain between 150 and 200 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from LDA_AandB.test_data_generator import simulate_corpus\n",
    "from LDA_AandB.lda_code import lda, group_docs\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set corpus parameters\n",
    "V = 100\n",
    "N_min = 150\n",
    "N_max = 200\n",
    "K = 2\n",
    "M = 10\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha_true = np.random.randint(1, 10, K)\n",
    "beta_true = np.random.randint(1, 10, V)\n",
    "\n",
    "# Generate simulated corpus\n",
    "bow, theta_true, phi_true = simulate_corpus(alpha_true, beta_true, M, N_min, N_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of our LDA depends on the choice of the hyperparameters $\\alpha$ and $\\beta$. The closer these hyperparameters are to the true values of the dataset, the better the algorithm's estimates of the topic and word distributions. \n",
    "\n",
    "When the hyperparameters $\\alpha$ and $\\beta$ are chosen to be the true values, our LDA algorithm captures the true topic distributions very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute error in topic probability estimates: 0.20089080229043943\n",
      "Documents labeled in group 1 : [0]\n",
      "Documents labeled in group 2 : [1 2 3 4 5 6 7 8 9]\n",
      "LDA document groups: None\n",
      "Documents labeled in group 1 : []\n",
      "Documents labeled in group 2 : [0 1 2 3 4 5 6 7 8 9]\n",
      "True document groups: None\n"
     ]
    }
   ],
   "source": [
    "# Train data on LDA implementation\n",
    "theta, phi = lda(bow, K, alpha_true, beta_true, 10000)\n",
    "print(\"Mean squared-error in topic probability estimates:\", np.mean((theta - theta_true)**2))\n",
    "print(\"LDA document groups:\", group_docs(theta, K))\n",
    "print(\"True document groups:\", group_docs(theta_true, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in real-world scenarios we don't know what the true values of $\\alpha$ and $\\beta$ are. In the case where the chosen hyperparameters are $\\textbf{not}$ the true values from the data, our LDA algorithm's estimates are less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-63ffe489d0da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train data on LDA implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average absolute error in topic probability estimates:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtheta_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LDA document groups:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"True document groups:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/STA663_Project/LDA_AandB/lda_code.py\u001b[0m in \u001b[0;36mlda\u001b[0;34m(bow, K, alpha, beta, n_iter)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# Run Gibbs sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mN_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgibbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# Estimate topic and word distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/STA663_Project/LDA_AandB/lda_code.py\u001b[0m in \u001b[0;36mgibbs\u001b[0;34m(w, K, M, V, doc_lens, alpha, beta, N_1, N_2, N_3, z, n_iter)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train data on LDA implementation\n",
    "theta, phi = lda(bow, K, 1, 1, 10000)\n",
    "print(\"Mean squared-error in topic probability estimates:\", np.mean((theta - theta_true)**2))\n",
    "print(\"LDA document groups:\", group_docs(theta))\n",
    "print(\"True document groups:\", group_docs(theta_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our LDA algorithm performs correctly, the simulated data testing illustrates how choosing proper prior parameters for the model can severly affect acccuracy in parameter estimation. Because of this, it is important to try different hyperparameters and perform sensitivity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications to Real Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply our algorithm to the Newsgroups corpus. This popular corpus contains documents from 30 different topics ranging from science to politics to religion.\n",
    "\n",
    "For our analysis we choose 15 randomly chosen documents from the Newsgroups corpus under the categories \"Computer Graphics\" and \"Christianity\". We then assess how accurately our LDA algorithm classifies these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from LDA_AandB.lda_code import lda, group_docs, get_key_words\n",
    "from LDA_AandB.test_data_generator import get_newsgroups, newsgroups_categories\n",
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Categories: ['soc.religion.christian', 'soc.religion.christian', 'comp.graphics', 'soc.religion.christian', 'comp.graphics', 'soc.religion.christian', 'soc.religion.christian', 'soc.religion.christian', 'comp.graphics', 'soc.religion.christian', 'soc.religion.christian', 'comp.graphics', 'soc.religion.christian', 'soc.religion.christian', 'comp.graphics']\n"
     ]
    }
   ],
   "source": [
    "cats = [newsgroups_categories[i] for i in [1, 15]]\n",
    "bow_news, labels, words = get_newsgroups(cats, 15)\n",
    "print(\"Document Categories:\", [cats[i] for i in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents labeled in group 1 : [ 0  1  3  5  6  7  8  9 10 11 12 13 14]\n",
      "Documents labeled in group 2 : [2 4]\n",
      "Key words for topic 1 :  ['accelerators', 'according', 'actions', 'actually', 'address', 'alive', 'allow', 'amiga', 'anonyomus', 'answer', 'ask', 'atheist', 'away', 'basis', 'behind', 'beset', 'beside', 'biblical', 'bigger', 'buy', 'cd', 'certainly', 'changing', 'cloudless', 'colour', 'commerical', 'copulating', 'copy', 'country', 'curious', 'curtain', 'declared', 'deforestation', 'demonstrated', 'demonstrates', 'descriptions', 'destroyed', 'determining', 'devastation', 'discovering', 'discussion', 'display', 'drilled', 'earth', 'email', 'encourage', 'evaluate', 'explanation', 'fail', 'finally', 'flames', 'forget', 'ftp', 'fulfil', 'generation', 'genesis', 'genuineness', 'grace', 'guess', 'hearts', 'heaven', 'hebrews', 'hi', 'holds', 'holy', 'honest', 'iff', 'important', 'indistinguishable', 'individual', 'innocent', 'instructions', 'interior', 'ironic', 'james', 'judge', 'justified', 'khoros', 'killed', 'licking', 'likely', 'listed', 'login', 'looks', 'lot', 'louis', 'mankind', 'mapped', 'mark', 'member', 'men', 'mercy', 'moslems', 'needless', 'noone', 'obeyed', 'okay', 'ours', 'package', 'password', 'pentacostal', 'performing', 'physical', 'place', 'polluted', 'precision', 'previous', 'punished', 'qualification', 'reagan', 'real', 'realize', 'recommend', 'redwoods', 'related', 'religious', 'renewing', 'resolved', 'resurrection', 'retail', 'sarah', 'second', 'sell', 'send', 'shall', 'similar', 'skepticism', 'skin', 'slaveowner', 'sleeps', 'sometimes', 'sort', 'stephen', 'steve', 'strict', 'students', 'subdue', 'summarised', 'supposedly', 'suspend', 'table', 'teach', 'thanks', 'themselves', 'told', 'try', 'type', 'undoubtedly', 'university', 'used', 'video', 'visuals', 'walked', 'wars', 'wave', 'while', 'white', 'willing', 'witnesses']\n",
      "Key words for topic 2 :  ['able', 'about', 'absolute', 'absolutely', 'absurdity', 'accept', 'accordance', 'accurate', 'across', 'acting', 'action', 'acts', 'add', 'adding', 'admit', 'adulterous', 'again', 'against', 'all', 'allows', 'alone', 'also', 'although', 'always', 'amazing', 'amounts', 'and', 'another', 'answering', 'answers', 'any', 'anything', 'apathetic', 'appear', 'appears', 'are', 'area', 'argument', 'around', 'arrogant', 'as', 'assume', 'assumptions', 'astray', 'at', 'average', 'back', 'based', 'be', 'beatific', 'because', 'become', 'becomes', 'been', 'before', 'behalf', 'being', 'beings', 'beliefs', 'believe', 'believers', 'believes', 'bellarmine', 'best', 'between', 'beyond', 'bible', 'biblically', 'bishop', 'bit', 'black', 'body', 'books', 'both', 'bought', 'brain', 'brains', 'buddhist', 'burned', 'but', 'by', 'call', 'called', 'came', 'canalways', 'cannot', 'captialist', 'card', 'careful', 'carefully', 'cartisian', 'catholic', 'caused', 'certain', 'certainity', 'certianty', 'change', 'chaplain', 'chapter', 'choice', 'choose', 'christian', 'church', 'churches', 'claim', 'claimed', 'claiming', 'cobol', 'collect', 'color', 'come', 'comes', 'command', 'commentary', 'company', 'compared', 'compatible', 'computer', 'conclusion', 'condem', 'condemned', 'conditions', 'confident', 'constantly', 'constitute', 'consult', 'contemporary', 'contention', 'continues', 'convince', 'could', 'count', 'counter', 'couple', 'cover', 'criticised', 'culturally', 'curiosity', 'dead', 'decades', 'decide', 'decided', 'decisevely', 'defined', 'defining', 'deprive', 'detailed', 'did', 'difference', 'different', 'difficult', 'direct', 'direction', 'directly', 'discuss', 'discussing', 'discussions', 'dislike', 'displayed', 'dispute', 'do', 'doctor', 'doctrine', 'doctrines', 'dogma', 'doing', 'done', 'doubt', 'down', 'draw', 'dropped', 'during', 'earlier', 'earliest', 'early', 'either', 'elaborated', 'else', 'emancipated', 'eminently', 'end', 'english', 'enjoy', 'entering', 'enviorment', 'error', 'especially', 'eternity', 'european', 'evangelical', 'even', 'every', 'everyone', 'everything', 'evil', 'exact', 'exactly', 'examples', 'exhaust', 'existence', 'expect', 'experience', 'experiencing', 'explain', 'explained', 'explains', 'extent', 'face', 'fact', 'facts', 'failing', 'faith', 'fall', 'fallible', 'false', 'far', 'feet', 'few', 'fill', 'find', 'fine', 'first', 'flamers', 'flawless', 'fly', 'foolish', 'foot', 'form', 'former', 'found', 'francis', 'freedom', 'frustration', 'fundamental', 'further', 'gain', 'gave', 'gensis', 'genuine', 'get', 'gift', 'give', 'given', 'gives', 'giving', 'go', 'god', 'godly', 'going', 'good', 'gospel', 'got', 'grad', 'graphics', 'grasp', 'greatest', 'grosseteste', 'grossly', 'guarantee', 'guards', 'hair', 'hand', 'happen', 'hardly', 'have', 'he', 'head', 'heresy', 'high', 'highly', 'him', 'his', 'historical', 'hold', 'horses', 'human', 'icons', 'idea', 'if', 'impact', 'impicitly', 'implacably', 'implicit', 'implicitly', 'in', 'include', 'income', 'incorrectly', 'indicates', 'individuals', 'infallible', 'infested', 'inherently', 'insist', 'insistent', 'install', 'instead', 'insufferable', 'intelligent', 'intense', 'intentionally', 'interest', 'interested', 'interesting', 'interpret', 'into', 'intrigues', 'is', 'issue', 'it', 'its', 'itself', 'jesus', 'jews', 'john', 'judgment', 'just', 'keeping', 'killing', 'knew', 'know', 'knowing', 'knowledgable', 'knows', 'labled', 'language', 'last', 'latter', 'lay', 'lead', 'leaders', 'leads', 'leap', 'learn', 'lectures', 'legal', 'less', 'lesson', 'let', 'level', 'libertarian', 'life', 'limited', 'limits', 'literally', 'little', 'live', 'long', 'looked', 'loss', 'loyalty', 'ludwig', 'lying', 'magazine', 'major', 'make', 'makes', 'making', 'many', 'may', 'maybe', 'mean', 'means', 'mental', 'mete', 'mexico', 'middle', 'might', 'mistakes', 'mixed', 'modern', 'moment', 'more', 'most', 'moved', 'mr', 'much', 'murder', 'my', 'mystery', 'necessary', 'need', 'nephew', 'nephews', 'never', 'newsgroup', 'nietzche', 'no', 'none', 'not', 'note', 'nothing', 'notice', 'now', 'number', 'obedience', 'obey', 'objections', 'obvious', 'off', 'often', 'ok', 'omniscience', 'once', 'one', 'only', 'onto', 'open', 'opinion', 'opponents', 'opposed', 'order', 'original', 'other', 'ourselves', 'out', 'over', 'overrated', 'papacy', 'paraphrase', 'part', 'parted', 'particular', 'pascal', 'passage', 'past', 'pauline', 'pc', 'people', 'perfect', 'perhaps', 'person', 'personally', 'pharisaical', 'philosophy', 'philosphy', 'physically', 'pictures', 'placed', 'playmation', 'point', 'poorer', 'pope', 'popes', 'possibility', 'practice', 'predestination', 'preferring', 'pressure', 'presume', 'pretty', 'primary', 'principle', 'probable', 'probably', 'produces', 'product', 'profess', 'promise', 'prove', 'puffed', 'punish', 'punishment', 'put', 'quarrel', 'question', 'questioning', 'quite', 'ramble', 'ran', 'rape', 'rarely', 'rational', 'rationalism', 'rationality', 'read', 'reading', 'ready', 'reality', 'really', 'reason', 'reasonable', 'reasoning', 'recognise', 'recognising', 'recommended', 'record', 'recorded', 'records', 'refutations', 'reigning', 'reject', 'relevant', 'rely', 'remember', 'rendering', 'rest', 'result', 'revelation', 'righteousness', 'robert', 'said', 'salvation', 'save', 'saved', 'say', 'saying', 'says', 'schaeffer', 'schaeffers', 'scientist', 'scorched', 'scripture', 'secretary', 'secular', 'see', 'seemed', 'seeming', 'sense', 'sentence', 'sentences', 'seriously', 'set', 'shakier', 'sherlette', 'shortly', 'show', 'simple', 'simplest', 'sin', 'sinners', 'situations', 'slave', 'slavery', 'snag', 'so', 'some', 'somebody', 'someone', 'soon', 'sooner', 'sooooo', 'source', 'sources', 'speak', 'speaking', 'specific', 'specifically', 'spend', 'start', 'stated', 'statement', 'statues', 'step', 'stolen', 'stoning', 'stop', 'stopped', 'stops', 'struggling', 'student', 'successor', 'such', 'suggest', 'suggested', 'sums', 'superiority', 'supports', 'suppose', 'supposed', 'surely', 'taht', 'take', 'taken', 'talked', 'talking', 'tell', 'temple', 'testament', 'tested', 'the', 'their', 'them', 'then', 'theologians', 'theory', 'there', 'therefore', 'they', 'thing', 'things', 'think', 'thinking', 'thoroughly', 'though', 'tighten', 'too', 'took', 'totally', 'translations', 'treat', 'trouble', 'true', 'trust', 'trusted', 'truths', 'trying', 'two', 'unconditional', 'under', 'understand', 'understanding', 'unless', 'unlimited', 'unrelieved', 'until', 'upon', 'us', 'useless', 'using', 'valid', 'value', 'verified', 'very', 'violation', 'virtually', 'vision', 'voluntarily', 'walk', 'walking', 'wants', 'was', 'washing', 'ways', 'we', 'welcome', 'well', 'went', 'were', 'what', 'where', 'whether', 'wholeheartedly', 'whose', 'why', 'wife', 'wildly', 'will', 'window', 'witch', 'without', 'witnessing', 'woman', 'women', 'words', 'work', 'worries', 'worshipped', 'worthiness', 'would', 'wrecked', 'writing', 'written', 'writting', 'wrote', 'xian', 'xianity', 'yeah', 'year', 'years', 'yes', 'yet']\n"
     ]
    }
   ],
   "source": [
    "theta, phi = lda(bow_news, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_docs(theta, 2)\n",
    "labels\n",
    "get_key_words(phi, 1, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis with Competing Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare our LDA algorithm to an alternative approach to LDA and to another method of document classification called latent semantic analysis, or LSA. We use same simulated dataset used in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (Variational Bayes Approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn package in python implements LDA using the variational bayes approach as described in the original 2003 paper. The variational bayes approach introduces additional variational parameters to optimize. The algorithm minimizes the KL divergence between the posterior probability of the actual parameters and the posterior probability of the new variational parameters. The parameters are estimated using an Expectation-Maximization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = 2,\n",
    "                                random_state = 0)\n",
    "lda.fit(bow) \n",
    "LatentDirichletAllocation(...)\n",
    "\n",
    "lda.transform(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA is a different approach to classification than LDA. In essence, LSA is an application of a singular value decomposition, or SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "svd = TruncatedSVD(n_components = 2, n_iter = 7, random_state = 42)\n",
    "TruncatedSVD(algorithm = 'randomized', n_components = 2, n_iter = 7,\n",
    "        random_state = 42, tol = 0.0)\n",
    "\n",
    "svd.fit_transform(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison between the three algorithms shows that LSA performs the least accurately in estimating the true document distributions. LSA is the least complex of the three models, so it makes sense it performs more poorly than LDA.\n",
    "\n",
    "LDA implemented using the variational bayes approach appears to perform similarly to LDA implemented using the collapsed Gibbs sampler, even slightly better. This is likely because the LDA algorithm implemented in Sklearn does some form of parameter tuning/optimization for the hyperparameters, which improves its accuracy. In addition, the LDA algorithm under variational bayes performs much faster. This is likely because the variational bayes approach is faster computationally, as it doesn't need to iterate over all the words in the corpus like like in the Gibbs sampler approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion/Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA algorithm proves to be a powerful tool in classifying "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References/Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darling, W.M. (2011). A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling.\n",
    "\n",
    "David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res. 3 (March 2003), 993-1022."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
