{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Latent Dirichlet Allocation for Topic Modeling and Document Classification\n",
    "\n",
    "### Andrew Cooper, Brian Cozzi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we implement a form of Latent Dirichlet Allocation for topic modeling. Latent Dirichlet Allocation (LDA) was first introduced by Blei, Ng, and Jordan in 2003 as a hierarchical modeling approach for discrete data such as text in a corpus. This algorithm hinges on the notion that collections of data, such as text in a document, are generated according to a latent topic distribution, where each topic assigns probabilities to different words. The purpose of LDA in topic modeling is to group documents based on similar topic distributions, and to identify key words in each topic. Using a collapsed Gibbs sampler approach to LDA as described in Darling 2011, we implement an algorithm that estimates the latent topic distributions of a given corpus of documents. In addition, our algorithm returns key words assigned to each topic. We evaluate the performance of our algorithm on both simulated data and documents from the Newsgroups corpus.\n",
    "\n",
    "Key phrases: Latent Dirichlet Allocation, Topic Modeling, Collapsed Gibbs Sampler,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background (Brian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{State the research paper you are using. Describe the concept of the algorithm and why it is interesting and/or useful. If appropriate, describe the mathematical basis of the algorithm.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "Across a variety of disciplines, the simplification of large sets of categorical data are necessary to quantify similarities, detect anomialies between these large and superficially heterogeneous THINGS. \n",
    "- Applications of LDA beyond text \\\n",
    "\n",
    "Latent Dirichlet Allocation (LDA), introduced by Blei, Ng, and Jordan in their seminal 2003 paper, develops an intuitive framework through which these types of problems may be viewed. This original paper primarily focused on topic modeling as it pertains to text data. For consistency and in the interest of comprehensibility, will focus the remainder of this paper on LDA's application to text data. \n",
    "\n",
    "### How does this model work?\n",
    "The paper proposed a hierarchical Bayesian model for creating documents from words that are associated with a mixture of topics. \n",
    "- Definetti's theorem - exchangeability\n",
    "- LDA as a Hierarchical model\n",
    "\n",
    "### How can it be applied to our research\n",
    "Topic modeling and LDA is everywhere there is text data. Topic modeling for health records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages over other algorithms\n",
    "- LSI\n",
    "- pLSI\n",
    "\n",
    "### Disadvantages\n",
    "- difficult to implement\n",
    "- While it's not unique to this approach of topic modeling, it also neglects the word order. There are certain extensions of this problem to get around this. For example, rather than just considering occurrences of single words, we can also consider $n$ adjacent words (commonly called $n$-grams) to capture information about phrases. In this case too, the order of the $n$-grams is neglected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What will we do in this paper\n",
    "In this paper, we will describe our approach to implement LDA using a collapsed Gibbs sampler method described in Darling 2011. Section 2 will discuss the algorithm that has been implemented while Section 3 will provide an overview of how this algorithm was optimized. Sections 4 and 5 will demonstrate this algorithms application to simulated and  real-world datasets respectively. Section 6 will compare this algorithm to others $\\textbf{SPECIFY THIS LATER}$. We will conclude with a discussion in section 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Description of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA algorithm takes in 4 inputs: a corpus of documents, the number of topics, two optional choices for hyperparameters, and an optional specification on the number iterations for the Gibbs Sampler. The ultimate goal of the algorithm is to estimate the topic distribution for each document as well as the word distribution for each topic. It does this by making inference on the latent topics of each word in the given corpus. We perform this inference by implementing a Gibbs sampler. First the algorithm sets a starting point by randomly sampling topics for each of the words in the corpus. Then it iteratively samples new topics for each word using calculated posterior probabilities. After a number of iterations, the Gibbs Sampler returns the estimated topics for each word, which are then used to calculate the latent topic distributions and word distributions using Monte Carlo estimation. These estimated quantities are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA algorithm has many different symbols and components. We establish all the symbols used in our algorithm below:\n",
    "\n",
    "* $K$ = The number of topics\n",
    "* $M$ = The number of documents in the corpus\n",
    "* $N_m$ = The number of words in document $m$\n",
    "* $V$ = The number of possible words\n",
    "* $w_{m,n}$ = The nth word in document $m$\n",
    "* $z_{m,n}$ = The nth topic in document $m$ \n",
    "* $\\theta_m$ = The topic distribution of document $m$\n",
    "* $\\phi_k$ = The word distribution of topic $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Algorithm Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm takes as input a corpus of documents represented as a $M$x$V$  bag-of-words matrix. In addition, it takes as input the number of topics $K$. It also takes in two positive values representing the hyperparameters for the topic distribution ($\\alpha$) and the word distribution ($\\beta$). For this implementation we use symmetric priors for the dirichlet distributions, which means only one value is needed as input for each prior. Finally it takes as input the number iterations for the Gibbs sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for LDA has three main steps. The first step of the algorithm is preparing the data for the Gibbs sampler. As a starting point for our sampler, we first must randomly assign topics $z_{m,n}$ for each of the words in the given corpus. We then create two different count matrices, $N_1$ and $N_2$: $N_1$ is a $M$x$K$ matrix that counts the distribution of topics across documents. $N_2$ is a $K$x$V$ matrix that counts the distribution of words across topics. The count matrices are initialized according to the random topic assignment.\n",
    "\n",
    "The second step is the implementation of the Gibbs sampler. For each iteration, our sampler loops through every word $w_{m,n}$ in every document of our corpus. For each word, we first remove its assigned topic and decrement the appropriate count matrices $N_1$, $N_2$, and $N_3$. We then calculate the posterior probabilities of the word belonging to each of the possible topics. One of these topics is sampled using these probabilities and is assigned to the word. Finally, all of the count matrices are incremented according to this new topic. This process is done for all the words in the corpus for how many iterations the user specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gibbs Sampler Implementation***\n",
    "\n",
    "* Randomly assign topics $z_{m,n}$ for all words in corpus, and initialize count matrices $N_1$ and $N_2$\n",
    "\n",
    "* **for** $i = 1$ to n_iter:\n",
    "   * **for** $m = 1$ to $M$:\n",
    "       * **for** $n = 1$ to $N_m$:\n",
    "           * $w = w_{m,n}$\n",
    "           * $z_{old} = z_{m,n}$\n",
    "           * $N_1[m, z_{old}] -= 1$\n",
    "           * $N_2[z_{old}, w] -= 1$\n",
    "           * **for** $k = 1$ to $K$:\n",
    "               * $p_k = Pr(z = k | \\dots) \\propto (N_1[m, k] + \\alpha[k])*\\dfrac{N_2[k, w] + \\beta[w]}{\\sum_V{N_2[k, v] + \\beta[v]}}$\n",
    "           * Normalize $p$ \n",
    "           * $z_{new} = $ Sample from $Cat(K, p)$\n",
    "           * $z_{m,n} = z_{new}$\n",
    "           * $N_1[m, z_{new}] += 1$\n",
    "           * $N_2[z_{new}, w] += 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step to the algorithm is estimating and returning the quantities of interest. One quantity to estimate is the topic distribution $\\theta_m$ for each document $m$. Another quantity to estimate is the word distribution $\\phi_k$ for each topic. These quantities are estimated using the count matrices $N_1$ and $N_2$ established in the Gibbs sampler:\n",
    "\n",
    "$\\hat{\\theta}_{m,k} = \\dfrac{N_1[m, k] + \\alpha[k]}{\\sum_k{N_1[m, k] + \\alpha[k]}}$ \n",
    "\n",
    "$\\hat{\\phi}_{k, v} = \\dfrac{N_2[k, v] + \\beta[v]}{\\sum_v{N_2[k, v] + \\beta[v]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Description of Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the algorithm described above, we see that each iteration of the algorithm runs in $O(MNK)$ time. On its surface, this does not appear to scale well especially as more documents are added to the corpus. However, leveraging Python's interface with C and its ability to execute some loops in parallel, we can make the run time much more managable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-f1d107f99ac4>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-f1d107f99ac4>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    %%cython --compile-args=-fopenmp --link-args=-fopenmp --force\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Parallel execution with Cython\n",
    "\n",
    "#Will not work unless OpenMP is installed.\n",
    "\n",
    "%%cython --compile-args=-fopenmp --link-args=-fopenmp --force\n",
    "\n",
    "import cython\n",
    "from cython.parallel import parallel, prange\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def matrix_multiply2(double[:,:] u, double[:, :] v, double[:, :] res):\n",
    "    cdef int i, j, k\n",
    "    cdef int m, n, p\n",
    "\n",
    "    m = u.shape[0]\n",
    "    n = u.shape[1]\n",
    "    p = v.shape[1]\n",
    "\n",
    "    with cython.nogil, parallel():\n",
    "        for i in prange(m):\n",
    "            for j in prange(p):\n",
    "                res[i,j] = 0\n",
    "                for k in range(n):\n",
    "                    res[i,j] += u[i,k] * v[k,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-f3728385eed5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f3728385eed5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Parallelization with vectorize and guvectorize\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Parallelization with vectorize and guvectorize\n",
    "#----\n",
    "\n",
    "@numba.vectorize([float64(float64, float64),\n",
    "                  float32(float32, float32),\n",
    "                  float64(int64, int64),\n",
    "                  float32(int32, int32)],\n",
    "                 target='parallel')\n",
    "def f_parallel(x, y):\n",
    "    return np.sqrt(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the next section we will show that this code optimization offers a considerable speed up over the standard Python implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applications to Simulated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applications to Real Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- topic distribution by document\n",
    "- cluster topics (extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis with Competing Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion/Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Installation Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://packaging.python.org/tutorials/packaging-projects/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References/Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Darling, W.M. (2011). A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling.\n",
    "\n",
    "2) David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res. 3 (March 2003), 993-1022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
