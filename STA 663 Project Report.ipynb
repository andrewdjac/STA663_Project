{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Latent Dirichlet Allocation for Topic Modeling and Document Classification\n",
    "\n",
    "### Andrew Cooper, Brian Cozzi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report we implement a form of Latent Dirichlet Allocation for topic modeling. Latent Dirichlet Allocation (LDA) was first introduced by Blei, Ng, and Jordan in 2003 as a hierarchical modeling approach for discrete data such as text in a corpus. This algorithm hinges on the notion that collections of data, such as text in a document, are generated according to a latent topic distribution, where each topic assigns probabilities to different words. The purpose of LDA in topic modeling is to group documents based on similar topic distributions, and to identify key words in each topic. Using a collapsed Gibbs sampler approach to LDA as described in Darling 2011, we implement an algorithm that estimates the latent topic distributions of a given corpus of documents. In addition, our algorithm returns key words assigned to each topic. We evaluate the performance of our algorithm on both simulated data and documents from the Newsgroups corpus.\n",
    "\n",
    "Key phrases: Latent Dirichlet Allocation, Topic Modeling, Collapsed Gibbs Sampler,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper provides an overview of the implementation, optimization, and applications of Latent Dirichlet Allocation (LDA). LDA was first introduced in a seminal paper by Blei, Ng, and Jordan in 2003 as an attempt to hierarchically model discrete data such as text in a set of  documents. The original paper primarily focused on topic modeling as it pertains to text data and, for consistency and in the interest of comprehensibility, this is the application area where we will focus the remainder of this paper. In addition, rather than implementing the procedure with the Expectation Maximization approach outlined in Blei, Ng and Jordan, we will instead implement the inference using a collapsed Gibbs Sampler. \n",
    "\n",
    "LDA is a widely applicable topic modeling framework for a variety of disciplines. In fields of social and life sciences, the simplification of large sets of categorical data are necessary to quantify similarities and detect anomialies between these large and superficially heterogeneous objects. Perhaps the most abundant and accessible source of this type of categorical data comes from text. Researchers have been able to used in projects as disparate (or seemingly disparate) as finding topics in scientific literature (3) and personalizing medical recommendations (4). With over 20,000 citations (as of 30 April 2019) LDA has emerged as one of the most prolific and dominant methods for modeling topics in text across disciplines.\n",
    "\n",
    "LDA is a Bayesian hierarchical model with 3 \"levels\". The lowest is the (discrete) item level which, in an example using text, is a word. The model suggests that this item is produced by some latent \"topic\". Then, a finite number of these items are understood to be part of a collection (a document) making a collection a finite mixture of its constinuents' underlying topics. Over a set of these collections, each topic is modeled as an infinite mixture over topic probabilities. \n",
    "\n",
    "Despite its wide applicability, it is not without some minor drawbacks. While it's not unique to this approach of topic modeling, it neglects the word order in the documents. There are certain extensions of this problem to get around this. For example, rather than just considering occurrences of single words, we can also consider $n$ adjacent words (commonly called $n$-grams) to capture information about phrases. In this case too, the order of the $n$-grams is neglected.\n",
    "\n",
    "Additionally, in its original formulation, it can be very challenging to implement. In this paper, we follow the lead of several others who have chosen to implement LDA using a form of Gibbs sampling described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Formulation\n",
    "\n",
    "The primary challenge in LDA is posterior inference of the latent variables:$\\theta$, $\\phi$ and $z$. $\\theta$ represents a topic space for each document. $z_i$ is a representation of discrete topic. $\\phi$ represents the topic distributions for each word in the vocabulary, or more generally, the topic distribution for each discrete item in the set of all possible items.\n",
    "\n",
    "We are then tasked with computing\n",
    "\n",
    "$$p(\\theta, \\phi, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p(\\theta, \\phi, \\mathbf{z}, \\mathbf{w} | \\alpha, \\beta)}{p(\\mathbf{w} | \\alpha, \\beta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper by Blei, Ng, and Jordan (2003) used variational inference to calculate this. However it is also possible to address this using Gibbs Sampling where we can sample from the following conditional distributions:\n",
    "$$\\begin{align}\n",
    "\\theta_{t+1} \\sim & p(\\theta_{t} \\mid \\phi_t, \\mathbf{z}_t, w, \\alpha, \\beta) \\\\\n",
    "\\phi_{t+1} \\sim & p(\\phi_{t} \\mid \\theta_{t+1}, \\mathbf{z}_t, w, \\alpha, \\beta) \\\\\n",
    "\\mathbf{z}_{t+1} \\sim & p(\\mathbf{z}_t \\mid \\phi_{t+1}, \\theta_{t+1}, w, \\alpha, \\beta) \\\\ \\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darling (2011) shows how these can be expressed as a \"collapsed\" Gibbs sampler because $\\theta$ is just the set of document-topic proportions and $\\phi$ is set of topic-word distributions, these can be calculated using just the topic index assignments $\\mathbf{z}$. Therefore, we only need to sample from the full conditional of $\\mathbf{z}$ (after integrating out $\\phi$ and $\\theta$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we want to sample from $p(\\mathbf{w}, \\mathbf{z} | \\alpha, \\beta)$.\n",
    "\n",
    "We start with the full joint distribution of all latent parameters and set of words $\\mathbf{w}$ :$p(\\theta, \\phi, \\mathbf{z}, \\mathbf{w} | \\alpha, \\beta)$.\n",
    "\n",
    "$$\\begin{align} \n",
    "p(\\mathbf{w}, \\mathbf{z} | \\alpha, \\beta) =& \\int \\int p(\\mathbf{z}, \\mathbf{w}, \\theta, \\phi | \\alpha, \\beta) \\mathrm{d} \\theta \\mathrm{d} \\phi \\\\ \n",
    "=& \\int p(\\mathbf{z} | \\theta) p(\\theta | \\alpha) \\mathrm{d} \\theta \\int p\\left(\\mathbf{w} | \\phi_{z}\\right) p(\\phi | \\beta) \\mathrm{d} \\phi \\\\ \\end{align}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realize that $p(\\mathbf{z}| \\theta)$ and $p(\\mathbf{w} | \\phi_z)$ are multinomial and $p(\\theta| \\alpha$) and  $p(\\phi| \\beta)$ are Dirichlet priors. Therefore, we see that both terms inside the integrals have Dirichlet distributions. This yields the final expression:\n",
    "\n",
    "$$p(\\mathbf{w}, \\mathbf{z} | \\alpha, \\beta) = \\prod_{d} \\frac{B\\left(n_{d,}+\\alpha\\right)}{B(\\alpha)} \\prod_{k} \\frac{B\\left(n_{k,}+\\beta\\right)}{B(\\beta)}$$\n",
    "\n",
    "where B is the multinomial beta distribution, and $n_d$ and $n_k$ are the word counts by document and by topic respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, we will describe our approach to implement LDA using a collapsed Gibbs sampler method described in Darling 2011. Section 2 will discuss the algorithm that has been implemented while Section 3 will provide an overview of how this algorithm was optimized. Sections 4 and 5 will demonstrate this algorithms application to simulated and  real-world datasets respectively. Section 6 will compare this algorithm to others. We will conclude with a discussion in section 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Description of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA algorithm takes in 4 inputs: a corpus of documents, the number of topics, two optional choices for hyperparameters, and an optional specification on the number iterations for the Gibbs Sampler. The ultimate goal of the algorithm is to estimate the topic distribution for each document as well as the word distribution for each topic. It does this by making inference on the latent topics of each word in the given corpus. We perform this inference by implementing a Gibbs sampler. First the algorithm sets a starting point by randomly sampling topics for each of the words in the corpus. Then it iteratively samples new topics for each word using calculated posterior probabilities. After a number of iterations, the Gibbs Sampler returns the estimated topics for each word, which are then used to calculate the latent topic distributions and word distributions using Monte Carlo estimation. These estimated quantities are returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA algorithm has many different symbols and components. We establish all the symbols used in our algorithm below:\n",
    "\n",
    "* $K$ = The number of topics\n",
    "* $M$ = The number of documents in the corpus\n",
    "* $N_m$ = The number of words in document $m$\n",
    "* $V$ = The number of possible words\n",
    "* $w_{m,n}$ = The nth word in document $m$\n",
    "* $z_{m,n}$ = The nth topic in document $m$ \n",
    "* $\\theta_m$ = The topic distribution of document $m$\n",
    "* $\\phi_k$ = The word distribution of topic $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Algorithm Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm takes as input a corpus of documents represented as a $M$x$V$  bag-of-words matrix. In addition, it takes as input the number of topics $K$. It also takes in two positive values representing the hyperparameters for the topic distribution ($\\alpha$) and the word distribution ($\\beta$). For this implementation we use symmetric priors for the dirichlet distributions, which means only one value is needed as input for each prior. Finally it takes as input the number iterations for the Gibbs sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for LDA has three main steps. The first step of the algorithm is preparing the data for the Gibbs sampler. As a starting point for our sampler, we first must randomly assign topics $z_{m,n}$ for each of the words in the given corpus. We then create two different count matrices, $N_1$ and $N_2$: $N_1$ is a $M$x$K$ matrix that counts the distribution of topics across documents. $N_2$ is a $K$x$V$ matrix that counts the distribution of words across topics. The count matrices are initialized according to the random topic assignment.\n",
    "\n",
    "The second step is the implementation of the Gibbs sampler. For each iteration, our sampler loops through every word $w_{m,n}$ in every document of our corpus. For each word, we first remove its assigned topic and decrement the appropriate count matrices $N_1$, $N_2$, and $N_3$. We then calculate the posterior probabilities of the word belonging to each of the possible topics. One of these topics is sampled using these probabilities and is assigned to the word. Finally, all of the count matrices are incremented according to this new topic. This process is done for all the words in the corpus for how many iterations the user specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gibbs Sampler Implementation***\n",
    "\n",
    "* Randomly assign topics $z_{m,n}$ for all words in corpus, and initialize count matrices $N_1$ and $N_2$\n",
    "\n",
    "* **for** $i = 1$ to n_iter:\n",
    "   * **for** $m = 1$ to $M$:\n",
    "       * **for** $n = 1$ to $N_m$:\n",
    "           * $w = w_{m,n}$\n",
    "           * $z_{old} = z_{m,n}$\n",
    "           * $N_1[m, z_{old}] -= 1$\n",
    "           * $N_2[z_{old}, w] -= 1$\n",
    "           * **for** $k = 1$ to $K$:\n",
    "               * $p_k = Pr(z = k | \\dots) \\propto (N_1[m, k] + \\alpha[k])*\\dfrac{N_2[k, w] + \\beta[w]}{\\sum_V{N_2[k, v] + \\beta[v]}}$\n",
    "           * Normalize $p$ \n",
    "           * $z_{new} = $ Sample from $Cat(K, p)$\n",
    "           * $z_{m,n} = z_{new}$\n",
    "           * $N_1[m, z_{new}] += 1$\n",
    "           * $N_2[z_{new}, w] += 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step to the algorithm is estimating and returning the quantities of interest. One quantity to estimate is the topic distribution $\\theta_m$ for each document $m$. Another quantity to estimate is the word distribution $\\phi_k$ for each topic. These quantities are estimated using the count matrices $N_1$ and $N_2$ established in the Gibbs sampler:\n",
    "\n",
    "$\\hat{\\theta}_{m,k} = \\dfrac{N_1[m, k] + \\alpha[k]}{\\sum_k{N_1[m, k] + \\alpha[k]}}$ \n",
    "\n",
    "$\\hat{\\phi}_{k, v} = \\dfrac{N_2[k, v] + \\beta[v]}{\\sum_v{N_2[k, v] + \\beta[v]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Description of Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the algorithm described above, we see that each iteration of the algorithm runs in $O(MNK)$ time. On its surface, this does not appear to scale well especially as more documents, topics or words are added to the corpus. However, leveraging Python's interface with C and its ability to execute loops much faster than base Python, we can make the run time much more managable. \n",
    "\n",
    "It is also worth considering what type of optimization cannot be done. The most obvious limitation is in the outermost loop (Loop 1 in the pseudocode below). This is a Gibbs sampler which means that subsequent iterations of the sampler depend on previous iterations. Therefore, these iterations cannot be done in parallel.\n",
    "JIT nopython mode: A Numba compilation mode that generates code that does not access the Python C API. This compilation mode produces the highest performance code, but requires that the native types of all values in the function can be inferred. Unless otherwise instructed, the @jit decorator will automatically fall back to object mode if nopython mode cannot be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also worth consider what type of optimization cannot be done. The most obvious limitation is in the outermost loop. This is a Gibbs sampler which means that subsequent iterations of the sampler depend on previous iterations. Therefore, these iterations cannot be done in parallel.\n",
    "\n",
    "To investigate the remaining opportunities to optimize, we can consider the pseudocode below where we notice that there are 3 main loops for each iteration of the Gibbs sampler:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **for** $i = 1$ to n_iter: $\\boxed{\\textbf{LOOP 1}}$\n",
    "   * **for** $m = 1$ to $M$:  $\\boxed{\\textbf{LOOP 2}}$\n",
    "       * **for** $n = 1$ to $N_m$: $\\boxed{\\textbf{LOOP 3}}$\n",
    "           * $w = w_{m,n}$\n",
    "           * $z_{old} = z_{m,n}$\n",
    "           * $N_1[m, z_{old}] -= 1$\n",
    "           * $N_2[z_{old}, w] -= 1$\n",
    "           * **for** $k = 1$ to $K$: $\\boxed{\\textbf{LOOP 4}}$\n",
    "               * $p_k = Pr(z = k | \\dots) \\propto (N_1[m, k] + \\alpha[k])*\\dfrac{N_2[k, w] + \\beta[w]}{\\sum_V{N_2[k, v] + \\beta[v]}}$\n",
    "           * Normalize $p$ \n",
    "           * $z_{new} = $ Sample from $Cat(K, p)$\n",
    "           * $z_{m,n} = z_{new}$\n",
    "           * $N_1[m, z_{new}] += 1$\n",
    "           * $N_2[z_{new}, w] += 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop 4\n",
    "The task of the innermost loop (Loop 4) is to calculate the posterior probability of a topic given the observed count matrices. As noted, the equation provides the joint distribution, but not the normalizing constant. Inside this loop, it is clear that the values used as inputs are not changing over subsequent iterations: it is simply performing $K$ independent operations. This makes it a great candidate for parallelization however, using JIT for the remaining loops made this impossible. We will expand on this later.\n",
    "\n",
    "Instead, this loop was optimized using JIT and its _nopython_ option. Below, we provide initialization of a toy dataset and baseline timing for the Gibbs sampler:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Initialization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Values and Count matrices \n",
    "from LDA_AandB.Optimization_Examples import initialize, Gibbs\n",
    "M, doc_lens, Z, W, K, A, B, C, alpha, beta = initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Baseline Timing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5 ms ± 516 µs per loop (mean ± std. dev. of 30 runs, 30 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r30 -n30 Gibbs(M, doc_lens, Z, W, K, A, B, C, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Loop 4 Optimization Timing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.42 ms ± 447 µs per loop (mean ± std. dev. of 30 runs, 30 loops each)\n"
     ]
    }
   ],
   "source": [
    "from LDA_AandB.Optimization_Examples import Gibbs_faster\n",
    "%timeit -r30 -n30 Gibbs_faster(M, doc_lens, Z, W, K, A, B, C, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this timing that the optimization of the innermost loop resulted in a noticeable timing improvement from the original function. We now can turn our attention to loops over documents and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops 3 and 2\n",
    "At first glance, these loops might also seem like great candidates for parallelization. One might be inclined to think that the sampling of new topics happens independently of the document and word. Therefore, running these two loops simultaneously . However, upon closer inspection, notice that at each iteration the count matrices are updated based on the topics that are drawn. Specifically the matrix $N_2$ is updated in loop 3 which impacts the probabilities $p$ which, in turn, impacts the newly sampled topic and, thus, all count matrices. In other words $N_2$ is a corpus-level count matrix and this matrix is updated in loop 3. \n",
    "\n",
    "Despite this, we still optimize these loops using JIT and the _nopython_ option. Below we show the timing of this new function using the optimized loops 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop 2, 3, and 4 Optimization Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.55 ms ± 372 µs per loop (mean ± std. dev. of 30 runs, 30 loops each)\n"
     ]
    }
   ],
   "source": [
    "from LDA_AandB.Optimization_Examples import Gibbs_even_faster\n",
    "%timeit -r30 -n30 Gibbs_even_faster(M, doc_lens, Z, W, K, A, B, C, alpha, beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the timing in this case saw little improvement over the original. Still, we continue this and optimize the entire Gibbs sampler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges with \"nopython = true\"\n",
    "Despite the code's apparent similarity to native Python code, there were undoubtedly some complications caused by the _nopython_ option.\n",
    "\n",
    "One of the most noticeable complications with JIT was with the format of the original data: a dictionary. The first data format used for the word and topic matrices were, intuitively, dictionaries. In the dictionary, each key represented a document which had values corresponding to the discrete word or topic values. The fix was relatively straightforward, though likely less memory-efficient. These dictionaries were converted into matrices that had dimension $D \\times L_{max}$ where $L_{max}$ refers to the length of the longest document. Documents shorter than this were simply padded with zeros.\n",
    "\n",
    "Another challenge was parallelization. As noted above only Loop 4 was eligible for parallelization. However, in order to keep the remaining loops optimized using JIT, this could not be performed. One of the possible ideas for optimization was to vectorize Loop 4 using numba to execute the loops in parallel. However, calling one of these functions from JIT with the _nopython_ option gave an error.\n",
    " \n",
    "\n",
    "Another part of the function that had to be rewritten was the multinomial distribution which could not be called using _nopython_. Writing this function required the use of the Inverse CDF method to draw samples from the distribution.\n",
    "\n",
    "The function was supposed to take a probability vector $p$ and return an integer corresponding to the sampled topic based on the input probabilities. To do this, the helper function completed the following steps:\n",
    "1. Sample $U \\sim Unif(0,1)$\n",
    "2. Initialize cumulative probability $p$_sum$ = 0$\n",
    "3. Iterate over probability vector $p$, incrementing $p$_sum$ += p$  \n",
    "     $\\rightarrow$If at any point the value of the p_sum exceeds the value of U, return the index\n",
    "\n",
    "\n",
    "The index returns the sample from the multinomial distribution with probability vector $p$. This was also optimized using JIT _nopython_ and could not be parallelized for obvious reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applications to Simulated Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applications to Real Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- topic distribution by document\n",
    "- cluster topics (extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis with Competing Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion/Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Installation Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://packaging.python.org/tutorials/packaging-projects/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References/Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Darling, W.M. (2011). A Theoretical and Practical Implementation Tutorial on Topic Modeling and Gibbs Sampling.\n",
    "\n",
    "2) David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res. 3 (March 2003), 993-1022.\n",
    "\n",
    "3) T. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(suppl. 1):5228–5235, 2004.\n",
    "\n",
    "4) Zhang, Y., Chen, M., Huang, D., Wu, D., Li, Y., idoctor: Personalized and professionalized medical recommendations based on hybrid matrix factorization. Futur. Gener. Comput. Syst., 2016. http://dx.doi.org/10.1016/j.future.2015.12.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
